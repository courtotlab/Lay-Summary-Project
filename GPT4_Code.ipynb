{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc43f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "\n",
    "!python3 -m pip install pandas openpyxl metapub transformers torch torchvision tensorflow ragas==0.3.7 langchain-openai langchain openai tiktoken python-dotenv tf-keras scikit-learn biopython beautifulsoup4 lxml textstat openai nest_asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "from metapub import PubMedFetcher\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from functools import reduce\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "from ragas.metrics import faithfulness\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import textstat\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "import tqdm\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb1098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Loaded: ...20IA\n",
      "Entrez email set: t2yu@oicr.on.ca\n",
      "Entrez API key: 1e8a...\n",
      "Lay Summarizer Configuration: \n",
      " Number of articles: 1 \n",
      " Maximum word length: 350 \n",
      " Minimum word length: 250\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from your .env file, or export your API key\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"API key not found.\")\n",
    "else:\n",
    "    print(f\"API Key Loaded: ...{openai_api_key[-4:]}\")\n",
    "\n",
    "# OpenAI client\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Set location of configuration file, to load configuration variables\n",
    "with open(\"./config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "num_of_articles = config[\"num_of_articles\"]\n",
    "max_summary_length = config[\"max_summary_length\"]\n",
    "min_summary_length = config[\"min_summary_length\"]\n",
    "output_directory = config[\"output_directory\"]\n",
    "\n",
    "# Set an Entrez email within the config file\n",
    "fetch = PubMedFetcher()\n",
    "Entrez.email = config[\"entrez_email\"]\n",
    "Entrez.api_key = config[\"entrez_api_key\"]\n",
    "print(f\"Entrez email set: {Entrez.email}\")\n",
    "print(f\"Entrez API key: {Entrez.api_key[:4]}...\")\n",
    "print(f\"Lay Summarizer Configuration: \\n Number of articles: {num_of_articles} \\n Maximum word length: {max_summary_length} \\n Minimum word length: {min_summary_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c61905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize all functions\n",
    "# Fetch full texts \n",
    "def fetch_full_texts(pmids, fetcher):\n",
    "    time.sleep(1) \n",
    "    full_texts, titles = {}, {}\n",
    "\n",
    "    print(f\"Retrieved PMIDs: {pmids}\")\n",
    "    print(f\"Number of PMIDs retrieved: {len(pmids)}\")\n",
    "\n",
    "    for pmid in pmids:\n",
    "        try:\n",
    "            article = fetcher.article_by_pmid(pmid)\n",
    "            titles[pmid] = article.title\n",
    "            \n",
    "            pmcid = article.pmc\n",
    "            if pmcid:\n",
    "                handle = Entrez.efetch(db=\"pmc\", id=pmcid, rettype=\"xml\", retmode=\"xml\")\n",
    "                xml_data = handle.read()\n",
    "                handle.close()\n",
    "\n",
    "                soup = BeautifulSoup(xml_data, features=\"xml\")\n",
    "\n",
    "                paragraphs = soup.find_all(\"p\")\n",
    "                sections = soup.find_all(\"sec\")\n",
    "\n",
    "                text_chunks = [p.get_text() for p in paragraphs]\n",
    "                if not text_chunks and sections:\n",
    "                    # Fallback to extracting from section text\n",
    "                    text_chunks = [s.get_text() for s in sections]\n",
    "\n",
    "                full_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "                full_texts[pmid] = full_text if full_text.strip() else \"Full text unavailable\"\n",
    "                \n",
    "            else:\n",
    "                full_texts[pmid] = \"Full text unavailable\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching full text for PMID {pmid}: {e}\")\n",
    "            titles[pmid] = titles.get(pmid, \"Unavailable\")\n",
    "            full_texts[pmid] = \"Full text unavailable\"\n",
    "    return titles, full_texts\n",
    "    \n",
    "\n",
    "def fetch_abstracts(pmids, fetcher):\n",
    "    abstracts = {}\n",
    "    titles = {}\n",
    "    \n",
    "    for pmid in pmids:\n",
    "        try:\n",
    "            article = fetcher.article_by_pmid(pmid)\n",
    "            titles[pmid] = article.title\n",
    "            abstracts[pmid] = article.abstract if article.abstract else \"Abstract unavailable\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching abstract for PMID {pmid}: {e}\")\n",
    "            titles[pmid] = titles.get(pmid, \"Unavailable\")\n",
    "            abstracts[pmid] = \"Abstract unavailable\"\n",
    "    return titles, abstracts\n",
    "\n",
    "async_client = AsyncOpenAI(api_key=openai_api_key)\n",
    "\n",
    "async def async_summarize(pmid, text, max_length):\n",
    "    if not text or text == \"Full text unavailable\":\n",
    "        return pmid, \"No text available\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"Summarize this article in 250 to {max_length} words for a 2nd grade lay audience using simple words. \"\n",
    "        f\"The tone should be casual. Avoid emotive or negative language such as battle or fight. \"\n",
    "        f\"Keep words under 4 syllables and sentences under 10 words: {text}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = await async_client.chat.completions.create(\n",
    "            model=\"gpt-4o\", #\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes medical research.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            \n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        return pmid, content\n",
    "    except Exception as e:\n",
    "        return pmid, f\"Error summarizing: {e}\"\n",
    "\n",
    "async def gpt4_summarize_async(texts, max_length, concurrency=15):\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def limited_summary(pmid, text):\n",
    "        async with semaphore:\n",
    "            return await async_summarize(pmid, text, max_length)\n",
    "\n",
    "    tasks = [limited_summary(pmid, text) for pmid, text in texts.items()]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "def compute_readability(summary):\n",
    "    if summary and summary != \"No text available\":\n",
    "        flesch_score = textstat.flesch_reading_ease(summary)\n",
    "        grade_level = textstat.flesch_kincaid_grade(summary)\n",
    "        return flesch_score, grade_level\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def compute_ragas_faithfulness(summaries, references, prompt):\n",
    "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "    data = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": []\n",
    "    }\n",
    "    valid_pmids = []\n",
    "\n",
    "    for pmid, summary in summaries.items():\n",
    "        reference = references.get(pmid)\n",
    "        if (\n",
    "            summary \n",
    "            and reference \n",
    "            and summary != \"No text available\" \n",
    "            and reference != \"Full text unavailable\"\n",
    "        ):\n",
    "            data[\"question\"].append(prompt)\n",
    "            data[\"answer\"].append(summary)\n",
    "            data[\"contexts\"].append([reference])\n",
    "            valid_pmids.append(pmid)\n",
    "\n",
    "    \n",
    "    dataset = Dataset.from_dict(data)\n",
    "    results = evaluate(dataset, metrics=[faithfulness], llm=evaluator_llm)\n",
    "    return dict(zip(valid_pmids, results[\"faithfulness\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa9933",
   "metadata": {},
   "source": [
    "### Querying PubMed using keyword defined in 'config.json' \n",
    "Use the following section to fetch PMIDs based on the 'keyword' and 'number of articles' desired as defined in the config file. \n",
    "\n",
    "This will also create a .txt file with the PMIDs found for this query.\n",
    "\n",
    "*If you already have a list of PMIDs you would like to summarize, skip to the next step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f109742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section to get PMIDs via keyword query, defined in 'config.json'\n",
    "# NOTE: it will only fetch open access articles from PubMed\n",
    "if \"queries\" in config:\n",
    "\n",
    "    for query in config[\"queries\"]:\n",
    "            keyword = query[\"keyword\"]\n",
    "            output_file = query[\"output_file\"]\n",
    "\n",
    "            pmids = fetch.pmids_for_query(f\"{keyword} AND pubmed pmc open access[filter]\", retmax=num_of_articles)\n",
    "\n",
    "\n",
    "            print(pmids)\n",
    "\n",
    "            #writes the queried PMIDs to a txt file\n",
    "            with open(\"pmids_queried.txt\", \"w\") as f:\n",
    "                for pmid in pmids:\n",
    "                    f.write(pmid + \"\\n\")\n",
    "\n",
    "            \n",
    "else:\n",
    "    print(\"No queries found in the configuration file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6b6f9",
   "metadata": {},
   "source": [
    "### Loading PMIDs from a defined text file\n",
    "Use the following section if you already have a list of PMIDs you would like to summarize. This will load the PMIDs into the system from the .txt file ('pmid_file\") you have defined in 'config.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ad773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 PMIDs from pmid_file: \"pmids_queried.txt\"\n"
     ]
    }
   ],
   "source": [
    "# Use if you have a file of PMIDs to summarize and calculate metrics\n",
    "# Go to next section for querying PMIDs by keyword from PubMed\n",
    "if \"pmid_file\" in config:\n",
    "    pmid_file = config[\"pmid_file\"]\n",
    "    with open(pmid_file, \"r\") as f:\n",
    "        pmids = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Loaded {len(pmids)} PMIDs from pmid_file: \\\"{pmid_file}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main summarization section\n",
    "# Prints article information for pmids retrieved\n",
    "for pmid in pmids:\n",
    "    article = fetch.article_by_pmid(pmid)\n",
    "    print(f\"PMID {pmid}: {article.title}\")\n",
    "    print(f\"Authors: {', '.join([str(a) for a in article.authors])}\")\n",
    "    print(f\"Journal: {article.journal}\")\n",
    "    print(\"---\")\n",
    "\n",
    "#Generates fulltext-based summaries\n",
    "titles, full_texts = fetch_full_texts(pmids, fetch)\n",
    "filtered_texts = {pmid: text for pmid, text in full_texts.items() if text != \"Full text unavailable\"}\n",
    "full_text_summaries = await gpt4_summarize_async(filtered_texts, max_length=max_summary_length)\n",
    "# print(full_text_summaries)\n",
    "\n",
    "#Generates abstract-based summaries\n",
    "titles, abstracts = fetch_abstracts(pmids, fetch)\n",
    "abstract_summaries = await gpt4_summarize_async(abstracts, max_length=max_summary_length)\n",
    "# print(abstract_summaries)\n",
    "\n",
    "links = {pmid: f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" for pmid in pmids}\n",
    "\n",
    "\n",
    "# Calculates similarity between abstract vs full text summaries\n",
    "# Define the evaluate_similarity function\n",
    "def evaluate_similarity(abstract_summaries, full_text_summaries):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([abstract_summaries, full_text_summaries])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "similarity_scores = {\n",
    "    pmid: evaluate_similarity(abstract_summaries[pmid], full_text_summaries[pmid])\n",
    "    for pmid in abstract_summaries\n",
    "    if abstract_summaries[pmid] != \"No text available\"\n",
    "    and full_text_summaries.get(pmid)\n",
    "    and full_text_summaries[pmid] != \"No text available\"\n",
    "}\n",
    "\n",
    "# Calculate readability scores for abstract summaries\n",
    "abstract_readability = {\n",
    "    pmid: compute_readability(abstract_summaries[pmid])\n",
    "    for pmid in abstract_summaries\n",
    "}\n",
    "\n",
    "# Calculate readability scores for full text summaries\n",
    "full_text_readability = {\n",
    "    pmid: compute_readability(full_text_summaries[pmid])\n",
    "    for pmid in full_text_summaries\n",
    "}\n",
    "\n",
    "\n",
    "# Full text availability\n",
    "full_text_availability = {\n",
    "    pmid: \"Available\" if full_texts[pmid] != \"Full text unavailable\" else \"Unavailable\"\n",
    "    for pmid in pmids\n",
    "}\n",
    "\n",
    "# Define the prompt used for summarization\n",
    "ragas_prompt = (\"Summarize this article in 250 to 350 words words for an adult lay adult with 2nd grade level scientific experience. Use simple words. \"\n",
    "                \"The tone should be casual.\"\n",
    "                \"Avoid emotive or negative language such as battle or fight.\"\n",
    "                \"Keep words under 4 syllables and sentences under 10 words.\"\n",
    "                )\n",
    "\n",
    "# Compute RAGAS faithfulness\n",
    "abstract_ragas_faithfulness = compute_ragas_faithfulness(abstract_summaries, abstracts, ragas_prompt)\n",
    "fulltext_ragas_faithfulness = compute_ragas_faithfulness(full_text_summaries, full_texts, ragas_prompt)\n",
    "\n",
    "\n",
    "# Prepare DataFrames\n",
    "Title = pd.DataFrame(list(titles.items()), columns=[\"pmid\", \"Title\"])\n",
    "Link = pd.DataFrame(list(links.items()), columns=[\"pmid\", \"Link\"])\n",
    "Abstract = pd.DataFrame(list(abstracts.items()), columns=[\"pmid\", \"Abstract\"])\n",
    "Abstract = Abstract.assign(\n",
    "    Abstract=lambda df: df[\"Abstract\"].str.replace(\"\\n\", \" \")\n",
    ")\n",
    "FullTextFlag = pd.DataFrame(list(full_text_availability.items()), columns=[\"pmid\", \"FullText_Availability\"])\n",
    "Abstract_Summary = pd.DataFrame(list(abstract_summaries.items()), columns=[\"pmid\", \"Abstract_Summary\"])\n",
    "Full_Text_Summary = pd.DataFrame(list(full_text_summaries.items()), columns=[\"pmid\", \"Full_Text_Summary\"])\n",
    "Similarity = pd.DataFrame(list(similarity_scores.items()), columns=[\"pmid\", \"Abstract_vs_FullText_Similarity\"])\n",
    "\n",
    "Abstract_Readability = pd.DataFrame([\n",
    "    {\"pmid\": pmid, \n",
    "    \"Abstract_Flesch_Reading_Ease\": scores[0], \n",
    "    \"Abstract_Flesch_Kincaid_Grade\": scores[1]}\n",
    "    for pmid, scores in abstract_readability.items()\n",
    "])\n",
    "\n",
    "\n",
    "Full_Text_Readability = pd.DataFrame([\n",
    "    {\"pmid\": pmid, \n",
    "    \"FullText_Flesch_Reading_Ease\": scores[0], \n",
    "    \"FullText_Flesch_Kincaid_Grade\": scores[1]}\n",
    "    for pmid, scores in full_text_readability.items()\n",
    "])\n",
    "\n",
    "# RAGAS faithfulness scores\n",
    "Abstract_RAGAS_Faithfulness = pd.DataFrame(list(abstract_ragas_faithfulness.items()), columns=[\"pmid\", \"Abstract_RAGAS_Faithfulness\"])\n",
    "FullText_RAGAS_Faithfulness = pd.DataFrame(list(fulltext_ragas_faithfulness.items()), columns=[\"pmid\", \"FullText_RAGAS_Faithfulness\"])\n",
    "\n",
    "\n",
    "# Combine all DataFrames\n",
    "data_frames = [\n",
    "    Title, \n",
    "    Link, \n",
    "    Abstract, \n",
    "    FullTextFlag, \n",
    "    Abstract_Summary, \n",
    "    Full_Text_Summary, \n",
    "    Similarity,\n",
    "    Abstract_Readability,\n",
    "    Full_Text_Readability,\n",
    "    Abstract_RAGAS_Faithfulness,\n",
    "    FullText_RAGAS_Faithfulness\n",
    "]\n",
    "\n",
    "# Combine all DataFrames\n",
    "df_full = reduce(lambda left, right: pd.merge(left, right, on=\"pmid\", how=\"outer\"), data_frames)\n",
    "\n",
    "# Print Availability Counts\n",
    "print(f\"Full Text Availability for keyword '{keyword}':\")\n",
    "availability_counts = df_full[\"FullText_Availability\"].value_counts()\n",
    "print(\"Full Text Availability Counts:\")\n",
    "print(availability_counts)\n",
    "\n",
    "# Filter to keep only articles with full text available\n",
    "df_full = df_full[df_full[\"FullText_Availability\"] == \"Available\"]\n",
    "\n",
    "# Save the DataFrame to a CSV file with a timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_name, _ = os.path.splitext(output_file)\n",
    "output_file_with_timestamp = f\"{base_name}_{timestamp}.csv\"\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir = Path(output_directory)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "full_output_path = output_dir / output_file_with_timestamp\n",
    "df_full.to_csv(full_output_path, index=False)\n",
    "\n",
    "print(f\"Data for keyword '{keyword}' saved to: {output_file_with_timestamp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
